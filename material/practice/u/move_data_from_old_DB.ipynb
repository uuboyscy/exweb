{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected successfully!(1)\n"
     ]
    }
   ],
   "source": [
    "import pymysql\n",
    "\n",
    "# Source DB\n",
    "\n",
    "conn = pymysql.connect(\n",
    "    host = \"uuboyscymysql.clrhltpp3icl.ap-northeast-1.rds.amazonaws.com\",\n",
    "    port = int(3306),user = \"food\",\n",
    "    password = \"food\",\n",
    "    db = \"food\",\n",
    "    charset='utf8', \n",
    "    cursorclass=pymysql.cursors.DictCursor \n",
    "    )\n",
    "cursor = conn.cursor()\n",
    "print('Connected successfully!(1)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected successfully!(2)\n"
     ]
    }
   ],
   "source": [
    "# Target DB\n",
    "\n",
    "conn2 = pymysql.connect(\n",
    "    host = \"db\",\n",
    "    port = int(3306),user = \"uuboyscy\",\n",
    "    password = \"howdoyouturnthison\",\n",
    "    db = \"testdb\",\n",
    "    charset='utf8', \n",
    "    cursorclass=pymysql.cursors.DictCursor \n",
    "    )\n",
    "cursor2 = conn2.cursor()\n",
    "print('Connected successfully!(2)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "爬資料\n",
      "==================\n",
      "2\n",
      "報名Tibame課程的學生資料\n",
      "==================\n",
      "3\n",
      "把PTT上的文章標題、內容、推噓數蒐集起來\n",
      "==================\n",
      "4\n",
      "民調\n",
      "==================\n",
      "5\n",
      "研究什麼是最近的發燒話題\n",
      "==================\n",
      "6\n",
      "想了解青少年對那些事物有興趣，想抓取網路上青少年熱烈討論的議題。\n",
      "==================\n",
      "7\n",
      "選舉網路聲量的爬蟲\n",
      "==================\n",
      "8\n",
      "在寵物的討論版裡找飼養貓會遇到的問題\n",
      "==================\n",
      "9\n",
      "dcard裡抓特定圖案\n",
      "==================\n",
      "10\n",
      "爬股票的金融資訊，並統計與回測，找出最適合的投資標的\n",
      "==================\n",
      "11\n",
      "股價分析\n",
      "==================\n",
      "12\n",
      "到PTT電影版爬蟲抓取電影的討論熱度及網路聲量,分析電影受歡迎程度\n",
      "==================\n",
      "13\n",
      "租屋地址房東電話等等\n",
      "==================\n",
      "14\n",
      "股票分析。我認為老師上課很有活力也很盡力回答學生們的問題我覺得很棒!但可能因為我們是大數據班因此對於網頁概論只需要有初淺的了解 因此速度上有點偏快加上頁面真的很多所以有的時候會有些錯亂!但覺得你是一個用心的老師!很期待接下來的爬蟲課程!\n",
      "==================\n",
      "15\n",
      "運動賽事分析\n",
      "==================\n",
      "16\n",
      "爬蟲各大拍賣網站的商品與價格\n",
      "==================\n",
      "17\n",
      "爬蟲各大銀行換匯匯率\n",
      "==================\n",
      "18\n",
      "籃球運彩分析。 爬NBA各隊的每場得分及失分。要能區分主場跟客場的得失分資料。\n",
      "==================\n",
      "19\n",
      "需要整理大量資料的時候\n",
      "==================\n",
      "20\n",
      "想了解投資者對熱門股票的投資情形\n",
      "==================\n",
      "21\n",
      "股票或是經濟趨勢分析。應用較少，可以多舉實例解釋。\n",
      "==================\n",
      "22\n",
      "股市、氣象局\n",
      "==================\n",
      "()\n",
      "==================\n",
      "24\n",
      "2020年總統大選民調調查，爬資料以供數據整理\n",
      "==================\n",
      "()\n",
      "==================\n",
      "26\n",
      "在不同網站找同一商品的最優惠價格。\n",
      "==================\n",
      "27\n",
      "將104上的工作內容抓下來彙整，整理成表格，分析熱門的專長等等\n",
      "==================\n",
      "28\n",
      "找DMM上面的新番\n",
      "==================\n",
      "29\n",
      "統計數據\n",
      "==================\n",
      "30\n",
      "選舉人/汽車廠牌車型/歌手...之網路聲量，股票趨勢分析，專利地圖分析\n",
      "==================\n",
      "31\n",
      "統計搜尋某些項目的使用者愛好，分配適合的廣告給這些使用者觀看。\n",
      "==================\n",
      "32\n",
      "為了建立特定商業模型而需要爬取大量原始數據以供日後數據分析建模\n",
      "==================\n",
      "()\n",
      "==================\n",
      "()\n",
      "==================\n",
      "()\n",
      "==================\n",
      "()\n",
      "==================\n",
      "()\n",
      "==================\n",
      "()\n",
      "==================\n",
      "()\n",
      "==================\n",
      "()\n",
      "==================\n",
      "()\n",
      "==================\n",
      "()\n",
      "==================\n",
      "()\n",
      "==================\n",
      "()\n",
      "==================\n",
      "()\n",
      "==================\n",
      "()\n",
      "==================\n",
      "()\n",
      "==================\n",
      "()\n",
      "==================\n",
      "()\n",
      "==================\n"
     ]
    }
   ],
   "source": [
    "sql = 'SELECT * FROM food.tibame_db105 WHERE stnumber = \"%s\";'\n",
    "columns_name = ['stnumber', 'stname', 'stclass', 'q1', 'q2', 'q3', 'q4', 'q5', 'q6', 'q7', 'q8', 'q9', 'q10', 'stscore']\n",
    "\n",
    "for i in range(1, 50):\n",
    "    cursor.execute(sql%(i))\n",
    "    tmp = cursor.fetchall()\n",
    "    try:    \n",
    "        print(tmp[0]['stnumber'])\n",
    "        print(tmp[0]['q10'].replace('\\\\\\\\','\\\\').encode('utf-8').decode('unicode-escape'))\n",
    "    except:\n",
    "        print(tmp)\n",
    "    print('==================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "()\n",
      "==================\n",
      "1\n",
      "爬資料\n",
      "==================\n",
      "2\n",
      "報名Tibame課程的學生資料\n",
      "==================\n",
      "3\n",
      "把PTT上的文章標題、內容、推噓數蒐集起來\n",
      "==================\n",
      "4\n",
      "民調\n",
      "==================\n",
      "5\n",
      "研究什麼是最近的發燒話題\n",
      "==================\n",
      "6\n",
      "想了解青少年對那些事物有興趣，想抓取網路上青少年熱烈討論的議題。\n",
      "==================\n",
      "7\n",
      "選舉網路聲量的爬蟲\n",
      "==================\n",
      "8\n",
      "在寵物的討論版裡找飼養貓會遇到的問題\n",
      "==================\n",
      "9\n",
      "dcard裡抓特定圖案\n",
      "==================\n",
      "10\n",
      "爬股票的金融資訊，並統計與回測，找出最適合的投資標的\n",
      "==================\n",
      "11\n",
      "股價分析\n",
      "==================\n",
      "12\n",
      "到PTT電影版爬蟲抓取電影的討論熱度及網路聲量,分析電影受歡迎程度\n",
      "==================\n",
      "13\n",
      "租屋地址房東電話等等\n",
      "==================\n",
      "14\n",
      "股票分析。我認為老師上課很有活力也很盡力回答學生們的問題我覺得很棒!但可能因為我們是大數據班因此對於網頁概論只需要有初淺的了解 因此速度上有點偏快加上頁面真的很多所以有的時候會有些錯亂!但覺得你是一個用心的老師!很期待接下來的爬蟲課程!\n",
      "==================\n",
      "15\n",
      "運動賽事分析\n",
      "==================\n",
      "16\n",
      "爬蟲各大拍賣網站的商品與價格\n",
      "==================\n",
      "17\n",
      "爬蟲各大銀行換匯匯率\n",
      "==================\n",
      "18\n",
      "籃球運彩分析。 爬NBA各隊的每場得分及失分。要能區分主場跟客場的得失分資料。\n",
      "==================\n",
      "19\n",
      "需要整理大量資料的時候\n",
      "==================\n",
      "20\n",
      "想了解投資者對熱門股票的投資情形\n",
      "==================\n",
      "21\n",
      "股票或是經濟趨勢分析。應用較少，可以多舉實例解釋。\n",
      "==================\n",
      "22\n",
      "股市、氣象局\n",
      "==================\n",
      "23\n",
      "()\n",
      "==================\n",
      "24\n",
      "2020年總統大選民調調查，爬資料以供數據整理\n",
      "==================\n",
      "25\n",
      "()\n",
      "==================\n",
      "26\n",
      "在不同網站找同一商品的最優惠價格。\n",
      "==================\n",
      "27\n",
      "將104上的工作內容抓下來彙整，整理成表格，分析熱門的專長等等\n",
      "==================\n",
      "28\n",
      "找DMM上面的新番\n",
      "==================\n",
      "29\n",
      "統計數據\n",
      "==================\n",
      "30\n",
      "選舉人/汽車廠牌車型/歌手...之網路聲量，股票趨勢分析，專利地圖分析\n",
      "==================\n",
      "31\n",
      "統計搜尋某些項目的使用者愛好，分配適合的廣告給這些使用者觀看。\n",
      "==================\n",
      "32\n",
      "為了建立特定商業模型而需要爬取大量原始數據以供日後數據分析建模\n",
      "==================\n",
      "33\n",
      "()\n",
      "==================\n",
      "34\n",
      "()\n",
      "==================\n",
      "35\n",
      "()\n",
      "==================\n",
      "36\n",
      "()\n",
      "==================\n",
      "37\n",
      "()\n",
      "==================\n",
      "38\n",
      "()\n",
      "==================\n",
      "39\n",
      "()\n",
      "==================\n",
      "40\n",
      "()\n",
      "==================\n",
      "41\n",
      "()\n",
      "==================\n",
      "42\n",
      "()\n",
      "==================\n",
      "43\n",
      "()\n",
      "==================\n",
      "44\n",
      "()\n",
      "==================\n",
      "45\n",
      "()\n",
      "==================\n",
      "46\n",
      "()\n",
      "==================\n",
      "47\n",
      "()\n",
      "==================\n",
      "48\n",
      "()\n",
      "==================\n",
      "49\n",
      "()\n",
      "==================\n"
     ]
    }
   ],
   "source": [
    "sql = 'SELECT * FROM food.tibame_db105 WHERE stnumber = \"%s\";'\n",
    "insert_sql = 'INSERT INTO testdb.tibame SET stnumber=%s, stname=%s, stclass=%s, \\\n",
    "                               q1=%s, q2=%s, q3=%s, q4=%s, q5=%s, q6=%s, q7=%s, q8=%s, q9=%s, q10=%s, \\\n",
    "                               stscore=%s'\n",
    "columns_name = ['stnumber', 'stname', 'stclass', 'q1', 'q2', 'q3', 'q4', 'q5', 'q6', 'q7', 'q8', 'q9', 'q10', 'stscore']\n",
    "\n",
    "for stnumber in range(0, 50):\n",
    "    cursor.execute(sql%(stnumber))\n",
    "    tmp = cursor.fetchall()\n",
    "    try:    \n",
    "        print(tmp[0]['stnumber'])\n",
    "        print(tmp[0]['q10'].replace('\\\\\\\\','\\\\').encode('utf-8').decode('unicode-escape'))\n",
    "        cursor2.execute(insert_sql, (\n",
    "            tmp[0]['stnumber'],\n",
    "            tmp[0]['stname'].replace('\\\\\\\\','\\\\').encode('utf-8').decode('unicode-escape'),\n",
    "            tmp[0]['stclass'].upper(),\n",
    "            tmp[0]['q1'].replace('\\\\\\\\','\\\\').encode('utf-8').decode('unicode-escape'),\n",
    "            tmp[0]['q2'].replace('\\\\\\\\','\\\\').encode('utf-8').decode('unicode-escape'),\n",
    "            tmp[0]['q3'].replace('\\\\\\\\','\\\\').encode('utf-8').decode('unicode-escape'),\n",
    "            tmp[0]['q4'].replace('\\\\\\\\','\\\\').encode('utf-8').decode('unicode-escape'),\n",
    "            tmp[0]['q5'].replace('\\\\\\\\','\\\\').encode('utf-8').decode('unicode-escape'),\n",
    "            tmp[0]['q6'].replace('\\\\\\\\','\\\\').encode('utf-8').decode('unicode-escape'),\n",
    "            tmp[0]['q7'].replace('\\\\\\\\','\\\\').encode('utf-8').decode('unicode-escape'),\n",
    "            tmp[0]['q8'].replace('\\\\\\\\','\\\\').encode('utf-8').decode('unicode-escape'),\n",
    "            tmp[0]['q9'].replace('\\\\\\\\','\\\\').encode('utf-8').decode('unicode-escape'),\n",
    "            tmp[0]['q10'].replace('\\\\\\\\','\\\\').encode('utf-8').decode('unicode-escape'),\n",
    "            tmp[0]['stscore']\n",
    "        ))\n",
    "    except:\n",
    "        print(stnumber)\n",
    "        print(tmp)\n",
    "    print('==================')\n",
    "conn2.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.close()\n",
    "conn2.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected successfully!(1)\n"
     ]
    }
   ],
   "source": [
    "import pymysql\n",
    "\n",
    "# Source DB\n",
    "\n",
    "conn = pymysql.connect(\n",
    "    host = \"uuboyscymysql.clrhltpp3icl.ap-northeast-1.rds.amazonaws.com\",\n",
    "    port = int(3306),user = \"food\",\n",
    "    password = \"food\",\n",
    "    db = \"food\",\n",
    "    charset='utf8', \n",
    "    cursorclass=pymysql.cursors.DictCursor \n",
    "    )\n",
    "cursor = conn.cursor()\n",
    "print('Connected successfully!(1)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected successfully!(2)\n"
     ]
    }
   ],
   "source": [
    "# Target DB\n",
    "\n",
    "conn2 = pymysql.connect(\n",
    "    host = \"db\",\n",
    "    port = int(3306),user = \"uuboyscy\",\n",
    "    password = \"howdoyouturnthison\",\n",
    "    db = \"testdb\",\n",
    "    charset='utf8', \n",
    "    cursorclass=pymysql.cursors.DictCursor \n",
    "    )\n",
    "cursor2 = conn2.cursor()\n",
    "print('Connected successfully!(2)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "爬取證交所每日股價相關資訊, 將資料存至資料庫內, 讓機器學習的模組去學習使用, 以便後續作股價波動預測分析!\n",
      "==================\n",
      "2\n",
      "租屋資訊、二手車買賣網站、旅遊資訊、股市分析\n",
      "==================\n",
      "3\n",
      "蒐集輿情及網紅人物之網路正負面聲量、近期政經社會議題或事件對該網紅人物所帶來之正負面聲量影響；或是蒐集某熱銷網路商品之網路點擊率、點閱率及評價\n",
      "==================\n",
      "4\n",
      "專題:預測黃金價格，透過爬蟲取得過去金價蒐集資訊。\n",
      "==================\n",
      "5\n",
      "精品電商平台\n",
      "==================\n",
      "6\n",
      "推特投資法,尋找熱門標的或產業\n",
      "==================\n",
      "7\n",
      "po文較多的美食景點、演唱會資訊\n",
      "==================\n",
      "8\n",
      "職棒運彩結果預測，上網爬選手歷史資料，建立模型模型，預測下一場比賽對戰結果。\n",
      "==================\n",
      "9\n",
      "人流動線追蹤\n",
      "==================\n",
      "10\n",
      "找目前數據工程師的平均薪水\n",
      "==================\n",
      "11\n",
      "[{'stnumber': 11, 'stname': '\\\\\\\\u9673\\\\\\\\u6021\\\\\\\\u9716', 'stclass': 'DB104', 'q1': 'GET', 'q2': 'HTML', 'q3': 'POST', 'q4': '\\\\\\\\u6a19\\\\\\\\u7c64', 'q5': '\\\\\\\\u5c6c\\\\\\\\u6027', 'q6': '\\\\\\\\u5167\\\\\\\\u5bb9', 'q7': 'IMG', 'q8': 'CSS', 'q9': 'HEAD', 'q10': '\\\\\\\\u8f3f\\\\\\\\u60c5\\\\\\\\u71b1\\\\\\\\u5ea6\\\\\\\\u7684\\\\\\\\u5be6\\\\\\\\u52d9\\\\\\\\u5206\\\\\\\\u6790\\\\\\\\uff1a\\\\\\\\u4ee5\\\\\\\\u7279\\\\\\\\u5b9a\\\\\\\\u8b70\\\\\\\\u984c\\\\\\\\u3001\\\\\\\\u4eba\\\\\\\\u4e8b\\\\\\\\u7269\\\\\\\\uff082020 \\\\\\\\u5927\\\\\\\\u9078\\\\\\\\u3001\\\\\\\\u8a71\\\\\\\\u984c\\\\\\\\u7522\\\\\\\\u54c1\\\\\\\\uff09\\\\\\\\u9032\\\\\\\\u884c\\\\\\\\u95dc\\\\\\\\u9375\\\\\\\\u5b57\\\\\\\\u71b1\\\\\\\\u5ea6\\\\\\\\uff08\\\\\\\\u6b63\\\\\\\\u8ca0\\\\\\\\u8a55\\\\\\\\u3001\\\\\\\\u641c\\\\\\\\u5c0b\\\\\\\\uff0f\\\\\\\\u8a0e\\\\\\\\u8ad6\\\\\\\\u91cf\\\\\\\\uff09\\\\\\\\u7684\\\\\\\\u8072\\\\\\\\u91cf\\\\\\\\u5206\\\\\\\\u6790\\\\\\\\uff1b\\\\\\\\u85c9\\\\\\\\u7531\\\\\\\\u9019\\\\\\\\u4e9b\\\\\\\\u8cc7\\\\\\\\u6599\\\\\\\\u7d50\\\\\\\\u5408\\\\\\\\u300c\\\\\\\\u5546\\\\\\\\u696d\\\\\\\\u8f49\\\\\\\\u63db\\\\\\\\u7387\\\\\\\\u300d\\\\\\\\u505a\\\\\\\\u51fa\\\\\\\\u95dc\\\\\\\\u806f\\\\\\\\u6027\\\\\\\\u7814\\\\\\\\u7a76\\\\\\\\uff0c\\\\\\\\u5206\\\\\\\\u6790\\\\\\\\u8f3f\\\\\\\\u60c5\\\\\\\\u3001\\\\\\\\u7279\\\\\\\\u5b9a\\\\\\\\u64cd\\\\\\\\u4f5c\\\\\\\\uff08\\\\\\\\u4e8b\\\\\\\\u4ef6\\\\\\\\u767c\\\\\\\\u751f\\\\\\\\u3001\\\\\\\\u884c\\\\\\\\u92b7\\\\\\\\u624b\\\\\\\\u6cd5\\\\\\\\uff09\\\\\\\\u8207\\\\\\\\u5be6\\\\\\\\u969b\\\\\\\\u8f49\\\\\\\\u63db\\\\\\\\uff08\\\\\\\\u6c11\\\\\\\\u8abf\\\\\\\\u8072\\\\\\\\u91cf\\\\\\\\u3001\\\\\\\\u96fb\\\\\\\\u5546\\\\\\\\u8a02\\\\\\\\u55ae\\\\\\\\u91cf\\\\\\\\uff09\\\\\\\\u4e4b\\\\\\\\u9593\\\\\\\\u7684\\\\\\\\u95dc\\\\\\\\u806f\\\\\\\\uff0c\\\\\\\\u64ec\\\\\\\\u5b9a\\\\\\\\u5be6\\\\\\\\u7528\\\\\\\\u7684\\\\\\\\u64cd\\\\\\\\u4f5c\\\\\\\\u7b56\\\\\\\\u756', 'stscore': 10}]\n",
      "==================\n",
      "12\n",
      "\n",
      "==================\n",
      "13\n",
      "收集以往物價 預測未來物價發展\n",
      "==================\n",
      "14\n",
      "從醫院病例庫中或從小米運動的網頁，獲取個人的醫療訊息和運動訊息，分析預測該使用者的健康狀況，和未來可能發生的疾病隱憂\n",
      "==================\n",
      "15\n",
      "利用爬蟲抓取民眾開車習慣數據，分析後可以給予駕駛評分並建議，保險公司也能參考評分訂定保費\n",
      "==================\n",
      "16\n",
      "2019工作分類統計熱門行業有哪些\n",
      "==================\n",
      "17\n",
      "球員數據分析,生涯曲線,球隊戰力分析\n",
      "==================\n",
      "18\n",
      "全台星級旅館人氣分析\n",
      "==================\n",
      "19\n",
      "網絡資料收集\n",
      "==================\n",
      "20\n",
      "1.利用python追蹤川普推特,結合語意分析判別利空利多並串聯下單機直接放空或做多,若無法做到能解讀多空的語意分析,則可搭配市場波動度與方向決定順勢or逆勢2.蒐集個人個資,發文等資訊藉由大數據與NPL提供據統計基礎的職崖or選擇顧問EX:像你這樣口條清楚思路清晰高學歷的人中,樣本=652,E(錢 | 創業)=3000萬/年,E(s\n",
      "==================\n",
      "21\n",
      "醫療數據檢驗\n",
      "==================\n",
      "22\n",
      "在PTT八卦版或網路文章，蒐集2020總統大選的網路支持度，並用以預測開票結果，藉此來討論網路上和整體民眾實際開票差距的問題，能否增加關鍵字來更符合實際開票結果\n",
      "==================\n",
      "23\n",
      "交通局\n",
      "==================\n",
      "24\n",
      "消費者購買汽車會上網搜尋不同車廠生產的諸如性能, 配備, 外觀, 內裝, 單價 可建立一專題以讓消費者搜尋整合到最佳性價比車型貨車款.\n",
      "==================\n",
      "()\n",
      "==================\n",
      "26\n",
      "分析股市波動及交易量，找出趨勢和套用策略進行交易\n",
      "==================\n",
      "27\n",
      "收集地區在網路上的消費力，並分出高單價與低單價  進而找出適合切入的市場\n",
      "==================\n",
      "28\n",
      "去各大租屋網的成交行情 和附近的房價做相關比較 匯出全台房價行情\n",
      "==================\n",
      "29\n",
      "欲知某一社交網站中，最熱門的貼文為何?最常使用關鍵字是甚麼?長期觀測此網站一個月後，其中貼文討論的趨勢為何?\n",
      "==================\n",
      "30\n",
      "cjhxzhjk\\\n",
      "==================\n",
      "31\n",
      "<!--不知道> 氣象局:爬些氣象相關的內容、觀光局:抓各地小吃美食的介紹、其他部落格文章，借用其他人觀光的心得想法\n",
      "==================\n",
      "32\n",
      "空氣汙染結合工廠地址及地圖，需要爬蟲空氣汙染之各項數據及工廠地址\n",
      "==================\n",
      "33\n",
      "分析目前受歡迎的遊戲類型及內容，蒐集各大遊戲資訊站根據近期發售遊戲討論版內的討論量、正面及負面發言，各大遊戲評分網站給予的評分，各大直播、影片網站，有關遊戲影片的觀看人數及次數，去做各個遊戲的分數加總。\n",
      "==================\n",
      "()\n",
      "==================\n",
      "()\n",
      "==================\n",
      "()\n",
      "==================\n",
      "()\n",
      "==================\n",
      "()\n",
      "==================\n",
      "()\n",
      "==================\n",
      "()\n",
      "==================\n",
      "()\n",
      "==================\n",
      "()\n",
      "==================\n",
      "()\n",
      "==================\n",
      "()\n",
      "==================\n",
      "()\n",
      "==================\n",
      "()\n",
      "==================\n",
      "()\n",
      "==================\n",
      "()\n",
      "==================\n",
      "()\n",
      "==================\n"
     ]
    }
   ],
   "source": [
    "sql = 'SELECT * FROM food.tibame WHERE stnumber = \"%s\";'\n",
    "columns_name = ['stnumber', 'stname', 'stclass', 'q1', 'q2', 'q3', 'q4', 'q5', 'q6', 'q7', 'q8', 'q9', 'q10', 'stscore']\n",
    "\n",
    "for i in range(1, 50):\n",
    "    cursor.execute(sql%(i))\n",
    "    tmp = cursor.fetchall()\n",
    "    try:    \n",
    "        print(tmp[0]['stnumber'])\n",
    "        print(tmp[0]['q10'].replace('\\\\\\\\','\\\\').encode('utf-8').decode('unicode-escape'))\n",
    "    except:\n",
    "        print(tmp)\n",
    "    print('==================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stnumber\n",
      "11\n",
      "stname\n",
      "陳怡霖\n",
      "stclass\n",
      "DB104\n",
      "q1\n",
      "GET\n",
      "q2\n",
      "HTML\n",
      "q3\n",
      "POST\n",
      "q4\n",
      "標籤\n",
      "q5\n",
      "屬性\n",
      "q6\n",
      "內容\n",
      "q7\n",
      "IMG\n",
      "q8\n",
      "CSS\n",
      "q9\n",
      "HEAD\n",
      "q10\n",
      "\\\\u8f3f\\\\u60c5\\\\u71b1\\\\u5ea6\\\\u7684\\\\u5be6\\\\u52d9\\\\u5206\\\\u6790\\\\uff1a\\\\u4ee5\\\\u7279\\\\u5b9a\\\\u8b70\\\\u984c\\\\u3001\\\\u4eba\\\\u4e8b\\\\u7269\\\\uff082020 \\\\u5927\\\\u9078\\\\u3001\\\\u8a71\\\\u984c\\\\u7522\\\\u54c1\\\\uff09\\\\u9032\\\\u884c\\\\u95dc\\\\u9375\\\\u5b57\\\\u71b1\\\\u5ea6\\\\uff08\\\\u6b63\\\\u8ca0\\\\u8a55\\\\u3001\\\\u641c\\\\u5c0b\\\\uff0f\\\\u8a0e\\\\u8ad6\\\\u91cf\\\\uff09\\\\u7684\\\\u8072\\\\u91cf\\\\u5206\\\\u6790\\\\uff1b\\\\u85c9\\\\u7531\\\\u9019\\\\u4e9b\\\\u8cc7\\\\u6599\\\\u7d50\\\\u5408\\\\u300c\\\\u5546\\\\u696d\\\\u8f49\\\\u63db\\\\u7387\\\\u300d\\\\u505a\\\\u51fa\\\\u95dc\\\\u806f\\\\u6027\\\\u7814\\\\u7a76\\\\uff0c\\\\u5206\\\\u6790\\\\u8f3f\\\\u60c5\\\\u3001\\\\u7279\\\\u5b9a\\\\u64cd\\\\u4f5c\\\\uff08\\\\u4e8b\\\\u4ef6\\\\u767c\\\\u751f\\\\u3001\\\\u884c\\\\u92b7\\\\u624b\\\\u6cd5\\\\uff09\\\\u8207\\\\u5be6\\\\u969b\\\\u8f49\\\\u63db\\\\uff08\\\\u6c11\\\\u8abf\\\\u8072\\\\u91cf\\\\u3001\\\\u96fb\\\\u5546\\\\u8a02\\\\u55ae\\\\u91cf\\\\uff09\\\\u4e4b\\\\u9593\\\\u7684\\\\u95dc\\\\u806f\\\\uff0c\\\\u64ec\\\\u5b9a\\\\u5be6\\\\u7528\\\\u7684\\\\u64cd\\\\u4f5c\\\\u7b56\\\\u756\n",
      "stscore\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "cursor.execute(sql%(11))\n",
    "tmp = cursor.fetchall()\n",
    "for i in tmp[0]:\n",
    "    print(i)\n",
    "    try:\n",
    "        print(tmp[0][i].replace('\\\\\\\\','\\\\').encode('utf-8').decode('unicode-escape'))\n",
    "    except:\n",
    "        print(tmp[0][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\\\\\\\u8f3f\\\\\\\\u60c5\\\\\\\\u71b1\\\\\\\\u5ea6\\\\\\\\u7684\\\\\\\\u5be6\\\\\\\\u52d9\\\\\\\\u5206\\\\\\\\u6790\\\\\\\\uff1a\\\\\\\\u4ee5\\\\\\\\u7279\\\\\\\\u5b9a\\\\\\\\u8b70\\\\\\\\u984c\\\\\\\\u3001\\\\\\\\u4eba\\\\\\\\u4e8b\\\\\\\\u7269\\\\\\\\uff082020 \\\\\\\\u5927\\\\\\\\u9078\\\\\\\\u3001\\\\\\\\u8a71\\\\\\\\u984c\\\\\\\\u7522\\\\\\\\u54c1\\\\\\\\uff09\\\\\\\\u9032\\\\\\\\u884c\\\\\\\\u95dc\\\\\\\\u9375\\\\\\\\u5b57\\\\\\\\u71b1\\\\\\\\u5ea6\\\\\\\\uff08\\\\\\\\u6b63\\\\\\\\u8ca0\\\\\\\\u8a55\\\\\\\\u3001\\\\\\\\u641c\\\\\\\\u5c0b\\\\\\\\uff0f\\\\\\\\u8a0e\\\\\\\\u8ad6\\\\\\\\u91cf\\\\\\\\uff09\\\\\\\\u7684\\\\\\\\u8072\\\\\\\\u91cf\\\\\\\\u5206\\\\\\\\u6790\\\\\\\\uff1b\\\\\\\\u85c9\\\\\\\\u7531\\\\\\\\u9019\\\\\\\\u4e9b\\\\\\\\u8cc7\\\\\\\\u6599\\\\\\\\u7d50\\\\\\\\u5408\\\\\\\\u300c\\\\\\\\u5546\\\\\\\\u696d\\\\\\\\u8f49\\\\\\\\u63db\\\\\\\\u7387\\\\\\\\u300d\\\\\\\\u505a\\\\\\\\u51fa\\\\\\\\u95dc\\\\\\\\u806f\\\\\\\\u6027\\\\\\\\u7814\\\\\\\\u7a76\\\\\\\\uff0c\\\\\\\\u5206\\\\\\\\u6790\\\\\\\\u8f3f\\\\\\\\u60c5\\\\\\\\u3001\\\\\\\\u7279\\\\\\\\u5b9a\\\\\\\\u64cd\\\\\\\\u4f5c\\\\\\\\uff08\\\\\\\\u4e8b\\\\\\\\u4ef6\\\\\\\\u767c\\\\\\\\u751f\\\\\\\\u3001\\\\\\\\u884c\\\\\\\\u92b7\\\\\\\\u624b\\\\\\\\u6cd5\\\\\\\\uff09\\\\\\\\u8207\\\\\\\\u5be6\\\\\\\\u969b\\\\\\\\u8f49\\\\\\\\u63db\\\\\\\\uff08\\\\\\\\u6c11\\\\\\\\u8abf\\\\\\\\u8072\\\\\\\\u91cf\\\\\\\\u3001\\\\\\\\u96fb\\\\\\\\u5546\\\\\\\\u8a02\\\\\\\\u55ae\\\\\\\\u91cf\\\\\\\\uff09\\\\\\\\u4e4b\\\\\\\\u9593\\\\\\\\u7684\\\\\\\\u95dc\\\\\\\\u806f\\\\\\\\uff0c\\\\\\\\u64ec\\\\\\\\u5b9a\\\\\\\\u5be6\\\\\\\\u7528\\\\\\\\u7684\\\\\\\\u64cd\\\\\\\\u4f5c\\\\\\\\u7b56\\\\\\\\u756'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp[0]['q10']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'輿情熱度的實務分析：以特定議題、人事物（2020 大選、話題產品）進行關鍵字熱度（正負評、搜尋／討論量）的聲量分析；藉由這些資料結合「商業轉換率」做出關聯性研究，分析輿情、特定操作（事件發生、行銷手法）與實際轉換（民調聲量、電商訂單量）之間的關聯，擬定實用的操作策'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp[0]['q10'][0:-6].replace('\\\\\\\\','\\\\').encode('utf-8').decode('unicode-escape')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "()\n",
      "==================\n",
      "1\n",
      "爬取證交所每日股價相關資訊, 將資料存至資料庫內, 讓機器學習的模組去學習使用, 以便後續作股價波動預測分析!\n",
      "==================\n",
      "2\n",
      "租屋資訊、二手車買賣網站、旅遊資訊、股市分析\n",
      "==================\n",
      "3\n",
      "蒐集輿情及網紅人物之網路正負面聲量、近期政經社會議題或事件對該網紅人物所帶來之正負面聲量影響；或是蒐集某熱銷網路商品之網路點擊率、點閱率及評價\n",
      "==================\n",
      "4\n",
      "專題:預測黃金價格，透過爬蟲取得過去金價蒐集資訊。\n",
      "==================\n",
      "5\n",
      "精品電商平台\n",
      "==================\n",
      "6\n",
      "推特投資法,尋找熱門標的或產業\n",
      "==================\n",
      "7\n",
      "po文較多的美食景點、演唱會資訊\n",
      "==================\n",
      "8\n",
      "職棒運彩結果預測，上網爬選手歷史資料，建立模型模型，預測下一場比賽對戰結果。\n",
      "==================\n",
      "9\n",
      "人流動線追蹤\n",
      "==================\n",
      "10\n",
      "找目前數據工程師的平均薪水\n",
      "==================\n",
      "11\n",
      "輿情熱度的實務分析：以特定議題、人事物（2020 大選、話題產品）進行關鍵字熱度（正負評、搜尋／討論量）的聲量分析；藉由這些資料結合「商業轉換率」做出關聯性研究，分析輿情、特定操作（事件發生、行銷手法）與實際轉換（民調聲量、電商訂單量）之間的關聯，擬定實用的操作策\n",
      "==================\n",
      "12\n",
      "\n",
      "==================\n",
      "13\n",
      "收集以往物價 預測未來物價發展\n",
      "==================\n",
      "14\n",
      "從醫院病例庫中或從小米運動的網頁，獲取個人的醫療訊息和運動訊息，分析預測該使用者的健康狀況，和未來可能發生的疾病隱憂\n",
      "==================\n",
      "15\n",
      "利用爬蟲抓取民眾開車習慣數據，分析後可以給予駕駛評分並建議，保險公司也能參考評分訂定保費\n",
      "==================\n",
      "16\n",
      "2019工作分類統計熱門行業有哪些\n",
      "==================\n",
      "17\n",
      "球員數據分析,生涯曲線,球隊戰力分析\n",
      "==================\n",
      "18\n",
      "全台星級旅館人氣分析\n",
      "==================\n",
      "19\n",
      "網絡資料收集\n",
      "==================\n",
      "20\n",
      "1.利用python追蹤川普推特,結合語意分析判別利空利多並串聯下單機直接放空或做多,若無法做到能解讀多空的語意分析,則可搭配市場波動度與方向決定順勢or逆勢2.蒐集個人個資,發文等資訊藉由大數據與NPL提供據統計基礎的職崖or選擇顧問EX:像你這樣口條清楚思路清晰高學歷的人中,樣本=652,E(錢 | 創業)=3000萬/年,E(s\n",
      "==================\n",
      "21\n",
      "醫療數據檢驗\n",
      "==================\n",
      "22\n",
      "在PTT八卦版或網路文章，蒐集2020總統大選的網路支持度，並用以預測開票結果，藉此來討論網路上和整體民眾實際開票差距的問題，能否增加關鍵字來更符合實際開票結果\n",
      "==================\n",
      "23\n",
      "交通局\n",
      "==================\n",
      "24\n",
      "消費者購買汽車會上網搜尋不同車廠生產的諸如性能, 配備, 外觀, 內裝, 單價 可建立一專題以讓消費者搜尋整合到最佳性價比車型貨車款.\n",
      "==================\n",
      "25\n",
      "()\n",
      "==================\n",
      "26\n",
      "分析股市波動及交易量，找出趨勢和套用策略進行交易\n",
      "==================\n",
      "27\n",
      "收集地區在網路上的消費力，並分出高單價與低單價  進而找出適合切入的市場\n",
      "==================\n",
      "28\n",
      "去各大租屋網的成交行情 和附近的房價做相關比較 匯出全台房價行情\n",
      "==================\n",
      "29\n",
      "欲知某一社交網站中，最熱門的貼文為何?最常使用關鍵字是甚麼?長期觀測此網站一個月後，其中貼文討論的趨勢為何?\n",
      "==================\n",
      "30\n",
      "cjhxzhjk\\\n",
      "==================\n",
      "31\n",
      "<!--不知道> 氣象局:爬些氣象相關的內容、觀光局:抓各地小吃美食的介紹、其他部落格文章，借用其他人觀光的心得想法\n",
      "==================\n",
      "32\n",
      "空氣汙染結合工廠地址及地圖，需要爬蟲空氣汙染之各項數據及工廠地址\n",
      "==================\n",
      "33\n",
      "分析目前受歡迎的遊戲類型及內容，蒐集各大遊戲資訊站根據近期發售遊戲討論版內的討論量、正面及負面發言，各大遊戲評分網站給予的評分，各大直播、影片網站，有關遊戲影片的觀看人數及次數，去做各個遊戲的分數加總。\n",
      "==================\n",
      "34\n",
      "()\n",
      "==================\n",
      "35\n",
      "()\n",
      "==================\n",
      "36\n",
      "()\n",
      "==================\n",
      "37\n",
      "()\n",
      "==================\n",
      "38\n",
      "()\n",
      "==================\n",
      "39\n",
      "()\n",
      "==================\n",
      "40\n",
      "()\n",
      "==================\n",
      "41\n",
      "()\n",
      "==================\n",
      "42\n",
      "()\n",
      "==================\n",
      "43\n",
      "()\n",
      "==================\n",
      "44\n",
      "()\n",
      "==================\n",
      "45\n",
      "()\n",
      "==================\n",
      "46\n",
      "()\n",
      "==================\n",
      "47\n",
      "()\n",
      "==================\n",
      "48\n",
      "()\n",
      "==================\n",
      "49\n",
      "()\n",
      "==================\n"
     ]
    }
   ],
   "source": [
    "sql = 'SELECT * FROM food.tibame WHERE stnumber = \"%s\";'\n",
    "insert_sql = 'INSERT INTO testdb.tibame SET stnumber=%s, stname=%s, stclass=%s, \\\n",
    "                               q1=%s, q2=%s, q3=%s, q4=%s, q5=%s, q6=%s, q7=%s, q8=%s, q9=%s, q10=%s, \\\n",
    "                               stscore=%s'\n",
    "columns_name = ['stnumber', 'stname', 'stclass', 'q1', 'q2', 'q3', 'q4', 'q5', 'q6', 'q7', 'q8', 'q9', 'q10', 'stscore']\n",
    "\n",
    "for stnumber in range(0, 50):\n",
    "    cursor.execute(sql%(stnumber))\n",
    "    tmp = cursor.fetchall()\n",
    "    try:\n",
    "        if stnumber == 11:\n",
    "            print(tmp[0]['stnumber'])\n",
    "            print(tmp[0]['q10'][0:-6].replace('\\\\\\\\','\\\\').encode('utf-8').decode('unicode-escape'))\n",
    "            cursor2.execute(insert_sql, (\n",
    "                tmp[0]['stnumber'],\n",
    "                tmp[0]['stname'].replace('\\\\\\\\','\\\\').encode('utf-8').decode('unicode-escape'),\n",
    "                tmp[0]['stclass'].upper(),\n",
    "                tmp[0]['q1'].replace('\\\\\\\\','\\\\').encode('utf-8').decode('unicode-escape'),\n",
    "                tmp[0]['q2'].replace('\\\\\\\\','\\\\').encode('utf-8').decode('unicode-escape'),\n",
    "                tmp[0]['q3'].replace('\\\\\\\\','\\\\').encode('utf-8').decode('unicode-escape'),\n",
    "                tmp[0]['q4'].replace('\\\\\\\\','\\\\').encode('utf-8').decode('unicode-escape'),\n",
    "                tmp[0]['q5'].replace('\\\\\\\\','\\\\').encode('utf-8').decode('unicode-escape'),\n",
    "                tmp[0]['q6'].replace('\\\\\\\\','\\\\').encode('utf-8').decode('unicode-escape'),\n",
    "                tmp[0]['q7'].replace('\\\\\\\\','\\\\').encode('utf-8').decode('unicode-escape'),\n",
    "                tmp[0]['q8'].replace('\\\\\\\\','\\\\').encode('utf-8').decode('unicode-escape'),\n",
    "                tmp[0]['q9'].replace('\\\\\\\\','\\\\').encode('utf-8').decode('unicode-escape'),\n",
    "                tmp[0]['q10'][0:-6].replace('\\\\\\\\','\\\\').encode('utf-8').decode('unicode-escape'),\n",
    "                tmp[0]['stscore']\n",
    "            ))\n",
    "        else:\n",
    "            print(tmp[0]['stnumber'])\n",
    "            print(tmp[0]['q10'].replace('\\\\\\\\','\\\\').encode('utf-8').decode('unicode-escape'))\n",
    "            cursor2.execute(insert_sql, (\n",
    "                tmp[0]['stnumber'],\n",
    "                tmp[0]['stname'].replace('\\\\\\\\','\\\\').encode('utf-8').decode('unicode-escape'),\n",
    "                tmp[0]['stclass'].upper(),\n",
    "                tmp[0]['q1'].replace('\\\\\\\\','\\\\').encode('utf-8').decode('unicode-escape'),\n",
    "                tmp[0]['q2'].replace('\\\\\\\\','\\\\').encode('utf-8').decode('unicode-escape'),\n",
    "                tmp[0]['q3'].replace('\\\\\\\\','\\\\').encode('utf-8').decode('unicode-escape'),\n",
    "                tmp[0]['q4'].replace('\\\\\\\\','\\\\').encode('utf-8').decode('unicode-escape'),\n",
    "                tmp[0]['q5'].replace('\\\\\\\\','\\\\').encode('utf-8').decode('unicode-escape'),\n",
    "                tmp[0]['q6'].replace('\\\\\\\\','\\\\').encode('utf-8').decode('unicode-escape'),\n",
    "                tmp[0]['q7'].replace('\\\\\\\\','\\\\').encode('utf-8').decode('unicode-escape'),\n",
    "                tmp[0]['q8'].replace('\\\\\\\\','\\\\').encode('utf-8').decode('unicode-escape'),\n",
    "                tmp[0]['q9'].replace('\\\\\\\\','\\\\').encode('utf-8').decode('unicode-escape'),\n",
    "                tmp[0]['q10'].replace('\\\\\\\\','\\\\').encode('utf-8').decode('unicode-escape'),\n",
    "                tmp[0]['stscore']\n",
    "            ))\n",
    "    except:\n",
    "        print(stnumber)\n",
    "        print(tmp)\n",
    "    print('==================')\n",
    "conn2.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.close()\n",
    "conn2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REIxMDVEQjEwNQ==\n"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "a = base64.b64encode(b'DB105DB105').decode('ascii')\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DB105\n"
     ]
    }
   ],
   "source": [
    "b = base64.b64decode('REIxMDU=').decode('ascii')\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DB105\n"
     ]
    }
   ],
   "source": [
    "b = base64.b64decode('REIxMDU=').decode('ascii')\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 MQ== TVE9PQ==\n",
      "2 Mg== TWc9PQ==\n",
      "3 Mw== TXc9PQ==\n",
      "4 NA== TkE9PQ==\n",
      "5 NQ== TlE9PQ==\n",
      "6 Ng== Tmc9PQ==\n",
      "7 Nw== Tnc9PQ==\n",
      "8 OA== T0E9PQ==\n",
      "9 OQ== T1E9PQ==\n",
      "10 MTA= TVRBPQ==\n",
      "11 MTE= TVRFPQ==\n",
      "12 MTI= TVRJPQ==\n",
      "13 MTM= TVRNPQ==\n",
      "14 MTQ= TVRRPQ==\n",
      "15 MTU= TVRVPQ==\n",
      "16 MTY= TVRZPQ==\n",
      "17 MTc= TVRjPQ==\n",
      "18 MTg= TVRnPQ==\n",
      "19 MTk= TVRrPQ==\n",
      "20 MjA= TWpBPQ==\n",
      "21 MjE= TWpFPQ==\n",
      "22 MjI= TWpJPQ==\n",
      "23 MjM= TWpNPQ==\n",
      "24 MjQ= TWpRPQ==\n",
      "25 MjU= TWpVPQ==\n",
      "26 MjY= TWpZPQ==\n",
      "27 Mjc= TWpjPQ==\n",
      "28 Mjg= TWpnPQ==\n",
      "29 Mjk= TWprPQ==\n",
      "30 MzA= TXpBPQ==\n",
      "31 MzE= TXpFPQ==\n",
      "32 MzI= TXpJPQ==\n",
      "33 MzM= TXpNPQ==\n",
      "34 MzQ= TXpRPQ==\n",
      "35 MzU= TXpVPQ==\n",
      "36 MzY= TXpZPQ==\n",
      "37 Mzc= TXpjPQ==\n",
      "38 Mzg= TXpnPQ==\n",
      "39 Mzk= TXprPQ==\n",
      "40 NDA= TkRBPQ==\n",
      "41 NDE= TkRFPQ==\n",
      "42 NDI= TkRJPQ==\n",
      "43 NDM= TkRNPQ==\n",
      "44 NDQ= TkRRPQ==\n",
      "45 NDU= TkRVPQ==\n",
      "46 NDY= TkRZPQ==\n",
      "47 NDc= TkRjPQ==\n",
      "48 NDg= TkRnPQ==\n",
      "49 NDk= TkRrPQ==\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,50):\n",
    "    t = base64.b64encode(('%s'%(i)).encode()).decode('ascii')\n",
    "    q = base64.b64encode(('%s'%(t)).encode()).decode('ascii')\n",
    "    print(i, t, q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "random.randint(1,9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REIxMDU=\n",
      "REIxMDQ=\n"
     ]
    }
   ],
   "source": [
    "print(base64.b64encode(b'DB105').decode('ascii'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
